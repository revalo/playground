{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from experiments.npe.model import get_npe_model, get_npe_model_replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/threecircles_1.0.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_y',\n",
       " 'val_y',\n",
       " 'arr_0',\n",
       " 'arr_1',\n",
       " 'arr_2',\n",
       " 'arr_3',\n",
       " 'arr_4',\n",
       " 'arr_5',\n",
       " 'arr_6',\n",
       " 'arr_7',\n",
       " 'arr_8',\n",
       " 'arr_9']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "val_x = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_x.append(data['arr_%i' % (i)])\n",
    "    val_x.append(data['arr_%i' % (i + 5)])\n",
    "    \n",
    "train_y = data['train_y']\n",
    "val_y = data['val_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-361acd66317c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "train_x[4] = np.ones((3000000, 1))\n",
    "train_x[5] = np.ones((3000000, 1))\n",
    "train_x[6] = np.ones((3000000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown(X):\n",
    "    return [np.array([x[i] for x in X]) for i in range(len(X[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "def density_scatter( x , y, ax = None, sort = True, bins = 20, **kwargs )   :\n",
    "    \"\"\"\n",
    "    Scatter plot colored by 2d histogram\n",
    "    \"\"\"\n",
    "    if ax is None :\n",
    "        fig , ax = plt.subplots()\n",
    "    data , x_e, y_e = np.histogram2d( x, y, bins = bins)\n",
    "    z = interpn( ( 0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ) , data , np.vstack([x,y]).T , method = \"splinef2d\", bounds_error = False )\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    if sort :\n",
    "        idx = z.argsort()\n",
    "        x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax.scatter( x, y, c=z, **kwargs )\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = []\n",
    "py = []\n",
    "\n",
    "for t in range(100000):\n",
    "    px.append(train_x[0][t][4] + (train_x[0][t][6] + train_y[t][0]) / 10.0)\n",
    "    py.append(train_x[0][t][5] + (train_x[0][t][7] + train_y[t][1]) / 10.0)\n",
    "    \n",
    "    # Velocity\n",
    "#     px.append(points_n[0][0] - points_p[0][0])\n",
    "#     py.append(points_n[0][1] - points_p[0][1])\n",
    "    \n",
    "#     px.append(points_p[0][0])\n",
    "#     py.append(points_p[0][1])\n",
    "    \n",
    "density_scatter(np.array(px), np.array(py), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0][:, 1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_npe_model(max_pairs = 2, state_dim = 4, hidden_size = 256, variational = True)\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0003, clipnorm=1.0)\n",
    "model.compile(loss=lambda y, d: -d.log_prob(y), optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lossfunc(z_mux, z_muy, z_sx, z_sy, z_corr, x_data, y_data):\n",
    "    \"\"\"\n",
    "    Function to calculate given a 2D distribution over x and y, and target data\n",
    "    of observed x and y points\n",
    "    params:\n",
    "    z_mux : mean of the distribution in x\n",
    "    z_muy : mean of the distribution in y\n",
    "    z_sx : std dev of the distribution in x\n",
    "    z_sy : std dev of the distribution in y\n",
    "    z_rho : Correlation factor of the distribution\n",
    "    x_data : target x points\n",
    "    y_data : target y points\n",
    "    \"\"\"\n",
    "    step = tf.constant(1e-3, dtype=tf.float32, shape=(1, 1))\n",
    "\n",
    "    # Calculate the PDF of the data w.r.t to the distribution\n",
    "    result0_1 = tf_2d_normal(x_data, y_data, z_mux, z_muy, z_sx, z_sy, z_corr)\n",
    "    result0_2 = tf_2d_normal(\n",
    "        tf.add(x_data, step), y_data, z_mux, z_muy, z_sx, z_sy, z_corr\n",
    "    )\n",
    "    result0_3 = tf_2d_normal(\n",
    "        x_data, tf.add(y_data, step), z_mux, z_muy, z_sx, z_sy, z_corr\n",
    "    )\n",
    "    result0_4 = tf_2d_normal(\n",
    "        tf.add(x_data, step), tf.add(y_data, step), z_mux, z_muy, z_sx, z_sy, z_corr\n",
    "    )\n",
    "\n",
    "    result0 = tf.divide(\n",
    "        tf.add(tf.add(tf.add(result0_1, result0_2), result0_3), result0_4),\n",
    "        tf.constant(4.0, dtype=tf.float32, shape=(1, 1)),\n",
    "    )\n",
    "    result0 = tf.multiply(tf.multiply(result0, step), step)\n",
    "\n",
    "    # For numerical stability purposes\n",
    "    epsilon = 1e-20\n",
    "\n",
    "    # Apply the log operation\n",
    "    result1 = -tf.math.log(tf.maximum(result0, epsilon))  # Numerical stability\n",
    "\n",
    "    # Sum up all log probabilities for each data point\n",
    "    return tf.reduce_sum(result1)\n",
    "\n",
    "\n",
    "def tf_2d_normal(x, y, mux, muy, sx, sy, rho):\n",
    "    \"\"\"\n",
    "        Function that implements the PDF of a 2D normal distribution\n",
    "        params:\n",
    "        x : input x points\n",
    "        y : input y points\n",
    "        mux : mean of the distribution in x\n",
    "        muy : mean of the distribution in y\n",
    "        sx : std dev of the distribution in x\n",
    "        sy : std dev of the distribution in y\n",
    "        rho : Correlation factor of the distribution\n",
    "        \"\"\"\n",
    "    # eq 3 in the paper\n",
    "    # and eq 24 & 25 in Graves (2013)\n",
    "    # Calculate (x - mux) and (y-muy)\n",
    "    normx = tf.subtract(x, mux)\n",
    "    normy = tf.subtract(y, muy)\n",
    "    # Calculate sx*sy\n",
    "    sxsy = tf.multiply(sx, sy)\n",
    "    # Calculate the exponential factor\n",
    "    z = (\n",
    "        tf.square(tf.divide(normx, sx))\n",
    "        + tf.square(tf.divide(normy, sy))\n",
    "        - 2 * tf.divide(tf.multiply(rho, tf.multiply(normx, normy)), sxsy)\n",
    "    )\n",
    "    negRho = 1 - tf.square(rho)\n",
    "    # Numerator\n",
    "    result = tf.exp(tf.divide(-z, 2 * negRho))\n",
    "    # Normalization constant\n",
    "    denom = 2 * np.pi * tf.multiply(sxsy, tf.sqrt(negRho))\n",
    "    # Final PDF calculation\n",
    "    result = tf.divide(result, denom)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_coef(output):\n",
    "    # eq 20 -> 22 of Graves (2013)\n",
    "\n",
    "    z = output\n",
    "    \n",
    "    # Split the output into 5 parts corresponding to means, std devs and corr\n",
    "    z_mux, z_muy, z_sx, z_sy, z_corr = tf.split(z, 5, 1)\n",
    "\n",
    "    # The output must be exponentiated for the std devs\n",
    "    z_sx = tf.exp(z_sx)\n",
    "    z_sy = tf.exp(z_sy)\n",
    "    # Tanh applied to keep it in the range [-1, 1]\n",
    "    z_corr = tf.tanh(z_corr)\n",
    "\n",
    "    return [z_mux, z_muy, z_sx, z_sy, z_corr]\n",
    "\n",
    "\n",
    "def sample_gaussian_2d(mux, muy, sx, sy, rho):\n",
    "    \"\"\"\n",
    "    Function to sample a point from a given 2D normal distribution\n",
    "    params:\n",
    "    mux : mean of the distribution in x\n",
    "    muy : mean of the distribution in y\n",
    "    sx : std dev of the distribution in x\n",
    "    sy : std dev of the distribution in y\n",
    "    rho : Correlation factor of the distribution\n",
    "    \"\"\"\n",
    "    # Extract mean\n",
    "    mean = [mux, muy]\n",
    "    # Extract covariance matrix\n",
    "    cov = [[sx * sx, rho * sx * sy], [rho * sx * sy, sy * sy]]\n",
    "    # Sample a point from the multivariate normal distribution\n",
    "    x = np.random.multivariate_normal(mean, cov, 1)\n",
    "    return x[0][0], x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "model = get_npe_model(max_pairs = 2, state_dim = 4, hidden_size = 50, variational = True)\n",
    "\n",
    "def lossfunc(ytrue, ypred):\n",
    "    z_mux, z_muy, z_sx, z_sy, z_corr = get_coef(ypred)\n",
    "    return get_lossfunc(z_mux, z_muy, z_sx, z_sy, z_corr, ytrue[...,0], ytrue[...,1])\n",
    "\n",
    "def gaussian_nll(ytrue, ypreds):\n",
    "    \"\"\"Keras implmementation og multivariate Gaussian negative loglikelihood loss function. \n",
    "    This implementation implies diagonal covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: tf.tensor of shape [n_samples, n_dims]\n",
    "        ground truth values\n",
    "    ypreds: tf.tensor of shape [n_samples, n_dims*2]\n",
    "        predicted mu and logsigma values (e.g. by your neural network)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_log_likelihood: float\n",
    "        negative loglikelihood averaged over samples\n",
    "        \n",
    "    This loss can then be used as a target loss for any keras model, e.g.:\n",
    "        model.compile(loss=gaussian_nll, optimizer='Adam') \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_dims = int(int(ypreds.shape[1])/2)\n",
    "    mu = ypreds[:, 0:n_dims]\n",
    "    logsigma = ypreds[:, n_dims:]\n",
    "\n",
    "    mse = -0.5*tf.keras.backend.sum(tf.keras.backend.square((ytrue-mu)/tf.keras.backend.exp(logsigma)),axis=1)\n",
    "    sigma_trace = -tf.keras.backend.sum(logsigma, axis=1)\n",
    "    log2pi = -0.5*n_dims*np.log(2*np.pi)\n",
    "\n",
    "    log_likelihood = mse+sigma_trace+log2pi\n",
    "\n",
    "    return tf.keras.backend.mean(-log_likelihood)\n",
    "\n",
    "def mse2(ytrue, ypred):\n",
    "    return ((ytrue[...,0] - ypred[...,0])**2 + (ytrue[...,1] - ypred[...,1])**2) / 2\n",
    "\n",
    "def mse(ytrue, ypred):\n",
    "    z_mux, z_muy, z_sx, z_sy, z_corr = get_coef(ypred)\n",
    "    return ((ytrue[...,0] - z_mux)**2 + (ytrue[...,1] - z_muy)**2) / 2\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.003)\n",
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000000 samples, validate on 300000 samples\n",
      "Epoch 1/20\n",
      "3000000/3000000 [==============================] - 33s 11us/sample - loss: 0.0810 - val_loss: 0.0864\n",
      "Epoch 2/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0619 - val_loss: 0.0490\n",
      "Epoch 3/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0347 - val_loss: 0.0355\n",
      "Epoch 4/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0280 - val_loss: 0.0287\n",
      "Epoch 5/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0253 - val_loss: 0.0238\n",
      "Epoch 6/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0234 - val_loss: 0.0303\n",
      "Epoch 7/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0221 - val_loss: 0.0192\n",
      "Epoch 8/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0210 - val_loss: 0.0187\n",
      "Epoch 9/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0202 - val_loss: 0.0157\n",
      "Epoch 10/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0195 - val_loss: 0.0236\n",
      "Epoch 11/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 12/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 13/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0177 - val_loss: 0.0199\n",
      "Epoch 14/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0172 - val_loss: 0.0184\n",
      "Epoch 15/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0168 - val_loss: 0.0135\n",
      "Epoch 16/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0163 - val_loss: 0.0270\n",
      "Epoch 17/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0158 - val_loss: 0.0146\n",
      "Epoch 18/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0155 - val_loss: 0.0135\n",
      "Epoch 19/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0152 - val_loss: 0.0144\n",
      "Epoch 20/20\n",
      "3000000/3000000 [==============================] - 32s 11us/sample - loss: 0.0147 - val_loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=20, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23323942, -0.21783152])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model(val_x)\n",
    "p[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0003)\n",
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6deed6bf9fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model = tf.keras.models.load_model('model_zoo/npe_replicate_b1024_e50.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_simulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_simulation_variational\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# show_simulation_variational(model, length=15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/initphysics/experiments/npe/simulate.py\u001b[0m in \u001b[0;36mshow_simulation\u001b[0;34m(model, width, height, radius, length, past_steps, neighborhood_mask)\u001b[0m\n\u001b[1;32m     92\u001b[0m             )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Pixels / Second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \"\"\"\n\u001b[1;32m    728\u001b[0m     \u001b[0mcall_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# We will attempt to build a TF graph if & only if all inputs are symbolic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('model_zoo/npe_replicate_b1024_e50.h5')\n",
    "from experiments.npe.simulate import show_simulation, show_simulation_variational\n",
    "show_simulation(model)\n",
    "# show_simulation_variational(model, length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_zoo/stochastic_npe_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity_mse():\n",
    "    p = model.predict(val_x)\n",
    "    return (((np.vstack(val_x[:, 0])[:, 6:] + p) - (np.vstack(val_x[:, 0])[:, 6:] + val_y))**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_velocity_mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_zoo/npe_replicate_b1024_e50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_zoo/npe4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(b_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:, 0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.array(train_y):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-3]([train_x[0][41:42], train_x[2][41:42], np.array([[1.0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[4].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
